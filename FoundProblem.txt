# 25/08/28
- 在使用爬蟲找尋網頁的過程中，點擊按鈕後會記錄下這次發送的 request body。但是如果 API 包含 Token，同時這次自動化取得了 200 response，或許會讓這個 Token 失效，
進而導致後面重複使用 Token 的請求失敗。

    - 可能的解法

        - 使用 mitmproxy 看到 POST 請求後不送出，直接攔截下來。mitmproxy 可以自主回應 200 ok，這樣他就是還沒被用過的 Token。
            - (問)如果每個 API 能獨立做測試，這表示不需要前面瀏覽的狀態也能成功送出 API，那還大費周章建一棵樹的理由是甚麼?只要有API編號或可辨識字串就可以了吧?應該只需要API清單?
                - 這樣第二次做回測也就是察看 Response 時就有可以查閱的路徑，而且透過一條穩定的路由能夠快速產出完整的攻擊向量和報告

        - 在每次要送出請求時，都依照路由圖 run 一遍，產生全新的 Token 後在送出請求
            - (缺)實作上可能要依靠 Selenium 和 Requests 混用，感覺非常麻煩且很慢。
            - (優)這個能契合我所想要傳達的 Stateful 的概念，也才能凸顯 Api Tree 的功用所在。

    - 目前

        - 理論上 Token 沒被用過應該是能用的，而且網頁表單的輸入應該不會有太多是直接點擊可以 200 ok 的行為，我覺得先按照現有的演算法實作看看結果在下決定。
        如果能夠順利觸發的 API 很少，就要考慮改成用上面的解法實作。

# 25/09/04
- 我發現測到後面 Session 會過期，可能需要定期更換 Session
- 看能不能修改成 Ctrl + C 後一定會寫入 JSON

# 25/09/09
- 看能不能修改成 Ctrl + C 後一定會寫入 JSON [V]
- 我發現測到後面 Session 會過期，可能需要定期更換 Session (定期刷新的邏輯反而不好處理，在 Joomla admin pannel 可以設定 Session lifetime.) [?]
- admin pannel 的 installer 頁面不要動，這邊有太多插件是無關的  (多一個 function 設定是某個網頁不要動)
- 多一個功能是設定某些 API 不要動，例如 logout 的 api 不要動 [V]
- 在 Processing 處加上進度條 (已完成/未完成) 方便我查看目前進度。 (Grok 版有)
    - 有沒有可能加上 thread 加快速度 ?
- 在測試階段時點擊按鈕少了 required 值會不會可能降低能探索到的深度 (這好像無所謂)
- string index out of range (QQ)

- 目前主要問題 -> 樹狀結構建立問題 (應該要選擇一個 stable 的 cursor 或 grok 的版本，先確定可以正常掃描網站，然後自己改樹狀結構的寫入邏輯)
    1. 建立樹狀結構根結點選擇問題 (Wire.py 版本)
    2. 樹狀結構無法呈現全部的 urls，例如 admin pannel 裡面有一堆 url，但即使檢測到也掃描過 url，但就是沒辦法被記載入樹狀結構 (Grok.py 版本)


# 25/09/10

建立四個物件
- [V] VisitedUrl (set), 用來儲存走訪過什麼 url，不要出現環(原本的就可以)
- [V] ParentMap (dict), {url:parent} 可以在當前 url 在找尋網頁中的 <a> tag 時匯入，例如 index.php 網頁中有 article 就會是當 driver 在 /index.php 時匯入資訊 {'/article':'/index.php'}
- UrlToApis (dict), {url:apis:{}} 因為每個 url 底下的 api 不會具有深度的特性，所以直接另外取出來儲存。
- [V] UrlToNode (dict), {url:children} 這個在最後建樹階段時建立，是在未來方便查詢這個樹所建立的 index 表
                    {str:list}
- 建樹: 走訪 ParentMap，因為在掃描過程使用的是 BFS，所以 ParentMap 匯入的 Key 值順序會契合匯入的節點 Lavel，不會有提前需要匯入較深節點的問題，所以 Tree 會製作成
{
    rooturl:
        children: {
            suburl1: {
                children :{
                    subsuburl1 : {

                    }
                }
            }, 
            suburl2: {
                children :{
                    subsuburl1 :{

                    }
                }
            }
            ...
        }
}

在建樹時因為 ParentMap 的插入順序，會讓 suburl 層先被匯入(再搭配指向的 parent 可以確定應該將節點放在樹的何處)，
這時候就能在 UrlToNode 裡面建立第一個值 {root:[suburl1, suburl2]}，當 suburl1 都走過一次後會開始走到 suburl1 會建立為 {root:[suburl1, suburl2], suburl1:[]}
如此往復就能把 node 資訊完好無缺的存入 UrlToNode 裡面。

這邊只存 url 即可，因為 api 儲存僅收錄當前頁面中的，api，不需要考慮深度問題，所以僅需要一個扁平的 {url:{apis} }(裡面儲存的資料必須要包含整個 request body, header 等)

未來要查詢時就能透過 O(1) 的時間查詢到任何 url 之下的 children，同時能夠透過 url 查詢 api，甚至也能透過 ParentMap 查詢 previous 的 node。

(以上全部 url 皆已絕對路徑儲存，修改 Grok 那份 code 比較快，因為裡面還有 exclude url 的功能在)

目前 Grok 版本已經能正確輸出 tree.json，但還沒讓 Api 也 sync 起來

## 25/09/11 
- 把 API 的部分用之前的 AnalyseInput 和 Requests_handler 整理好再匯入 Json
    - 目前是 BFS 的方式，如果中途突然 get 出去抓 forms 的資訊可能導致邏輯出錯，到時候 bug 會修不完
    - 應該要嘗試如何把抓 API 的動作混合進目前的業務流程中。
    - 為了方便直接透過 url 檢索，結構應該要是這樣
    {
        url1 :{
            apiurl1: {
                'body':body.text,
                'method':method,
                'headers':headers
            },
            apiurl2: {
                ...
            }
        }
    }
    1. 先 init requests_handler, 給他 cookies/session 
    2. 我需要 AllTags, body_text, method, headers, url, 
    - 要插入 Payload 的產生邏輯要寫在 BuildData，那邊非常快速可以寫完

    -> 目前可以產收 API json，但是會有一些奇怪的情況，例如 req.body 是有東西的，但是進行 BuildData 時偵測 forms 卻偵測不到
        - 有可能那個頁面就是沒有 forms?
            -  有可能，例如 administrator 頁面一堆按鈕可以 POST 但自己本身沒有 forms
    - 目前 API 樹應該堪用了，但有疑問要解決
        - administrator 這邊真的沒有半個 API 需要紀錄?
        - 可能要跑看看 50 - 100 個結果再來確定是否正確

## 25/09/16
- 有沒有可能用 multithreads 加速?
    -> 有，但非常困難，因為原先使用 BFS，然後在 BFS 的特性中內涵樹狀結構的階層順序。如果改以 multithreads 
    可能會影響這個順序進而導致建樹階段出現錯誤。
    -> Python 內建的 queue.Queue 好像是 thread-safe，內建 lock 可以解決 multithreads 的問題

## 25/10/05
-> 目前考慮把 Requests 和 Selenium 模組分開來進行，爬蟲執行完後要把狀態交給 Requests 所以要把 Cookies Export 出來
    -> Load Successfully
-> 現在要測試注入無害的 requests，應該要忽略 login/logout
-? 如果有 404 的頁面是不是不要紀錄比較好 ?

## 25/10/07
-> 目前 Apis 清單內會抓到 login，這個應該要處理掉
-> Apis 清單內有一堆是空的，我猜是 not found 頁面很多是 POST，但是實際去看頁面後卻甚麼都抓不到，導致 apis.json 抓到很多空值
    -> 要把這些空值過濾掉 不然能使用的 Apis 太少
-> 目前有很多頁面是 not found 的，要看情況弄掉
    -> driver 可以吃到 status code 了，也不會記錄在 process 裡面，但是 Json 還是會記錄到，可能要思考一下這樣是好還是不好
-> 發現新問題！！！！，如果執行過某個API後才新增網頁呢? 那這個網頁要怎麼確認?
    -> 目前 Submit an article 後就會出現新的頁面在外面
-> Fuzzable 有的時候能正常記錄在 Apis.json, 但有時候又不行，不確定為何

-> 有時候沒辦法登入有時候可以，本來以為是 headers 的問題，後來發現 headers 隨便抓到 google 的也能登入，表示跟 headers 無關。我猜是 Session 有沒有過期導致的。
    -> 修改 System 那邊的 Session lifetime

************
當務之急是先讓 Requests 能夠正常發出，先嘗試自動化使用 submit-an-article
    -> submit-an-article 那個的 api 裡面要是 task.save 才能順利發出文章
        -> 但是 task.save 的 apis 有包含在我的 apis 清單裡面，理論上不用修改其他內容
        -> 原本發現 Fuzzable 不能正確標上去，現在標上去了。
        -> 目前的版本可以從 Crawler 接到 Apis.json 後透過 requests 成功送出請求

## 25/10/08

目前發現執行爬蟲後為了檢測 status code，導致部分 requests 無法正常地被 driver.get 捕捉到(因為僅用弱的 time.sleep())
這樣會讓一些網頁檢測不到 driver.requests 進而出現錯誤。
-> 不對啊完蛋，我發現 404 的頁面出來的原因應該是因為 urljoin 出錯，/administrator/index.php 的頁面的 href 直接合併到了 /index.php 後面了
```
path:  http://192.168.11.129:8080/administrator/index.php
t:  index.php?option=com_joomlaupdate
url: http://192.168.11.129:8080/index.php?option=com_joomlaupdate
path:  http://192.168.11.129:8080/administrator/index.php
t:  index.php?option=com_joomlaupdate
url: http://192.168.11.129:8080/index.php?option=com_joomlaupdate
```
joomla 在搞= =
-> 目前是當作 Joomla 的 Special case 處理，如果未來延伸到要用到其他系統上要注意，邏輯寫在 GetMergeUrl 處

Processing (done/pending) 121/182:   http://192.168.11.129:8080/administrator/index.php?option=com_users&view=debuggroup&group_id=6
[9716:15964:1008/141347.490:ERROR:ui\latency\latency_info.cc:112] CompositorFrameSinkSupport::MaybeSubmitCompositorFrame, LatencyInfo vector size 667 is too big.

目前在 Processing 到 121 時會沒辦法 saving，就嘗試用 118 吧
在 118 的時候會有一個很詭異的錯誤
Reached limit 118, saving and exiting...
Driver closed.
Save failed: 'http://192.168.11.129:8080/administrator/index.php?option=com_admin&view=profile&layout=edit&id=255'

-> 在 50 筆的時候有部分 body 的解碼錯了QQ
    -> 好像是因為 GetNext 太快了，我改回 time.sleep(3) 就正常回來了

## 25/10/15

118 還是怪怪的 Save failed: 'http://192.168.11.129:8080/administrator/index.php?option=com_admin&view=profile&layout=edit&id=759'
不知道是甚麼問題，改決可以只瀏覽這個網頁看看? 之後再來修這個問題

今天先來想無害字串
1. 要能夠盡可能快速的對應出在 API Tree 的哪個位子，以及哪個欄位
    -> url & form field 
2. 如果用 Hash 又會需要另外存一個 map (耗空間，但或許是最後的方案)
3. 使用 base64 編碼 url + field (url 部份去掉 scheme 和 domain)
    -> (本身屬於無損編碼，但會讓整體的長度拉長許多，如果 url 還有帶 token 可能會導致長度過長)
    -> (但是 base64 字元不包含 ><'"&%* 這類可能觸發防火牆或其他錯誤的符號，這會大幅度的加快注入工作，也能直接還原不需要 map)
    -> 唯一擔心的是長度限制

-> MTSEC_Hash_FieldID
    -> MTSEC 是可辨識的前綴
    -
    > Hash 可以用來快速定位至樹上的某個 node，但是具體要用多長的 Hash 需要考慮一下
        -> 記錄前 8 位 hash 值或是更多?
        -> 目前可能可以先針對整陀 url 進行 Hash 然後用這個 url 結果
            -> 不過這樣會有很嚴重的泛用性問題
                -> 光是在爬蟲搜尋的途中 就可能因為大量的相似網站導致耗時過長，因為同樣頁面會有一樣問題才對
                -> 過大的狀態也會加深遇到碰撞的機率
                    -> 如果只取 8 位，則可能在 65536 筆時出現 50% 的碰撞機率 (md5)
                    -> 12 位的話是 1670萬筆資料才可能會有 50% 的碰撞機率(md5)
        -> 注入時只取前面 8 or 12 位，但是存入 json 內存完整的 hash，
            -> 那是不是該儲存成 list 而非 dict 呢  
                -> 但是 List 查找平均情況是 O(n)
    
    -> FieldID 的部分也要用方便辨識的字串，首先假設注入 body list 如下
        -> jform[title]:
        -> jform[task]:
        -> jform[...]:
        那就依序從上到下，title 為第一個, task 為第二個以此類推。
        然後也要使用好辨識字串，例如 Hash_Fidm01 是 title，這樣就可以快速 mapping 到每一個 input field 

## 25/10/21
前面的 Save failed 的問題解決了，好像是因為在 GetNext 檢測 status code 的時候被 skip 掉，導致少記錄到。總之寫入的時候察看一下即可

-> 目前也已經完成注入的字串部分，接著要思考怎麼把樹狀結構加上 hash 讓他好辨識
-> 還有怎麼把過程送往 OWASP ZAP 比較恰當
    -> 不知道 OWASP ZAP 能不能吃這種 input
-> 我發現有些 body 好像不能正確執行 BuildData，有些像下面這樣沒被整理過，正確的格式應該要是第二個

    - "body": "jform%5Btitle%5D=&jform%5Balias%5D=&jform%5Btype%5D=component&jform%5Blink%5D= ...
    - "body": {
        "client_id": "0",
        "menutype": "",
        "filter[search]": "Fuzzable",
        "list[fullordering]": "a.lft ASC",
        ...
        }
    
    這會導致目前正在進行的把 Hash 和 form 的對應號寫入的動作出錯，如果要完成 Hash 部分的話這邊要先改好

-> 不然先研究 OWASP ZAP 吧
    -> 好像不太能完整的照著我的邏輯做，他沒這麼彈性好用
    -> FuzzingBook 不知道有沒有戲
        -> 超級麻煩= = 要下載 msbuild, MSVC, 和 graphviz, 然後 pip install 時會一直卡在 pygraphviz，這個要直接到 PyPl 找 tar.gz 抓下來裝，然後才能 pip install fuzzingbook

## 25/10/23
-> fuzzingbook 有進展，可以產出很多 XSS Payload 了。
    -> 但目前的問題是很多 HTML Event 其實不好觸發 XSS，目前先保留，之後可能要回來研究 Event 如何觸發 Script
    -> 初版的 fuzzingbook 應該完工了

-> 目前 HashtoAPI 還沒完工 (Crawler.py)


## 25/10/28
identifiable string -> istring
-> 目前在執行把網頁上出現可辨識字串的 url 記錄起來
    -> 用 regex 可以精準地進行辨識，但是為了提高漏洞注入成功率，或許需要連注入點在哪個 tag 都找出來
        -> 目前已經把 url 上面哪裡有 istring 找了出來，包含他是甚麼 tag
    -> 在自動尋找的過程中出現 403 API 嘗試瀏覽外部連結
        -> 應該是因為 requests 會自動進行 redirect，但是 redirect 可能會到外部連結而沒有權限
            -> 算解決了
    
    -> 在 traverse/main.py 中已經把 get 相關的 api 都在檢索一遍，同時取出底下出現 istring 的 url，目前 100 個 requests 裡面有 23 個 fuzzable 的目標

-> 現在需要把 Crawler 的儲存多加上一個 ApiHash，依靠 Api 的 Hash Key 指到特定的 Api，裡面還要包含 body 的參數列表方便 Fuzzer 使用
    -> 目前在 Crawler 有些 body 沒辦法正確解析body 內的資訊。
    -> 部分 API 還是無法分析出 Fuzzable 的參數
        -> http://192.168.11.129:8080/administrator/index.php?option=com_users&view=group&layout=edit  (找的到)
        -> http://192.168.11.129:8080/administrator/index.php?option=com_users&view=level&layout=edit (找不到)
        -> 不然抓這兩個網頁的 html source code 出來看好了，直接把這兩個 source code 求 AllTags 然後匯入 GetInputInfo 看看問題在哪
        -> http://192.168.11.129:8080/index.php/site-settings 這個好像也應該找到 Fuzzable 才對
            -> 有沒有可能是其實都有找到 只是 BuildData 漏了資料

# 25/10/29
-> 對的，其實都有找到只是網站有多個同名 form 的時候抓到的資料會出錯。
    -> 解法應該是將所有 form 欄位都整合成特大表單
        -> 如果有垃圾元素呢 例如網站中的 SearchWord 會不會造成 Fuzzing 效果或 Fuzzable string 辨識出錯。。
        -> 我發現同樣的 action 是因為前端可能會按不同的畫面，然後 JS 會 render 出不同的頁面，這種時候 forms 底下的內容就會改，如果後端 function 吃的參數
        固定的話，搞不好 action 合併後能觸發的 success 更少，應該要維持合併前的作法才對

        -> 如果硬在 joomla 上面實作這種精準的抓取 tag 的動作，可能會導致切換到其他 web 系統時失靈，因為原始碼在撰寫上沒有一定的 pattern 限制，所以精準判斷
        意義不大。我認為改以 resourcePool 儲存參數結果應該會更好，resourcePool 儲存所有參數是否 Fuzzable 的結果，然後我要注入或者 BuildData 時直接查詢
        應該會更好，同時理論上也能兼容更多的 Web 系統

# 25/11/05
-> 目前暫時使用 argsPool 把參數的精度補上一些
-> 但是出現 redirect 問題，是之前為了解決 redirect 時改的 code，送出 requests 之後會出錯
    -> task 錯了，我賦值那邊讓他全部統一了
    -> 有的時候怪怪的 先用舊版的 requests_handler，然後 administrator 那邊 session 要把 db 和 php 的都啟用

# 25/11/06
先來實驗有 argParse.py 和沒有的區別
-> crawler 裡面 process 100 個 url
    -> Total result found: 355 (ArgParse)
        -> 但這次的 container 送過蠻多次 injectharmless，不知道有沒有區別
    -> Total result found: 104 (No ArgParse)
        -> 可能要在測試一次，前面有亂掉的地方~''~


-> 接下來要實作在網頁中找到 Fuzzable 的 API 的 Hash Map，存 Full Hash 還是 Hash key(12位)?
    -> 12 位的 hash 要達到碰撞機率 50% 需要的 k 為 16777216 個 API，一般系統中不可能出現那麼多
        -> 如果一個網站只有 1000 個 API，那麼發生碰撞的機率僅約為 1.776*10^-9，可忽略不計。
    -> 直接用 12 位 hash 當作 Key 來紀錄資料即可
    -> 在爬蟲階段就紀錄 API，因為如果再 injectharmless 階段會有太多不必要的寫入操作 

reflectUrl 可以 mapping 到 apis.json 的 key，從這個 value 底下找 hash12 相同的來用就好，
目前的 code 可能可以往下，但是我的 Apis 和 reflectMap 是不同情形下產生的，所以直接跑會出錯，需要再跑一次 Crawler

    {
        apiHash : {
            hash12 : {
                apiUrl: "" 
                body : {
                    Fuzzable or original value
                }
                hash40 : ""
                reflectUrl : []
            }
        }
    }

*******
問題: 
- 這麼多 Fuzzable 參數，我應該對每個參數都獨立進行 Fuzzing 還是一次一坨一起
    -> 一次一坨的話如果因為特定參數導致 req 被擋掉怎麼辦
    -> 一次一個的話要測多少遍，該不該留下來

- 我發現雖然是不同的 API，但很可能都是相同參數在被 Fuzz 譬如 jform[title]，但不確定會不會是個案，需不需要將這種參數記錄起來 ?
*******


## 
先看一下上一次的 這邊做完後趕快去把 FuzzingBook 連起來

## 25/11/12

哭啊，有一個網頁從 /index.php 看到是 http://192.168.11.129:8080/index.php?task=article.edit&a_id=1&return=aHR0cDovLzE5Mi4xNjguMTEuMTI5OjgwODAvaW5kZXgucGhw
連過去後 current 會變成 http://192.168.11.129:8080/index.php?view=form&layout=edit&a_id=1&return=aHR0cDovLzE5Mi4xNjguMTEuMTI5OjgwODAvaW5kZXgucGhw

這導致後面的 traverse 的動作做不起來，因為會出現 key error，問題的原因是 request 和 response 的 url 不同，這可能是內部有涉及 redirect 的處理
    -> 因為我後面的連結都是透過 url 進行 key mapping，這個影響是致命的，必須想辦法解掉QQ
    -> 目前暫無法處理這個問題，只能先手動修改 url 的數值讓兩邊能 match

哭啊，不是每個網頁上的 iString 來源 API 都在該網頁下，所以用 apis[key] 去做 mapping 根本找不到，還是得回頭用 hash12 做一個 key-value 的 dictionary

依照上次的 apiHash:{} 結構，我的檢測方式應該是只針對這些API的Fuzzable輸入進行檢測
- 打開 apiHash:{} 結構，然後遍歷 hash12
-> {
    hash12: {
        apiUrl:
        body: {
            網頁中找到的改成 fuzzable
            其餘留原樣
        }
        header:
        method
    }
   }
然後這些 hash12 全部跑一次後從 reflectMap 裡面讀 url 確認就好了

- 找出網頁中全部的 hash12 然後沒有在裡面的直接把 key 拔掉
    - 目前 hashSet 已經過濾出所有能用的 API，接下來要把 hashes 裡面不存在 hashSet 中的 key 拔掉
    - 拔完之後就能遍歷 reflectMap 然後改 hashes 裡面的 body，最後就能直接拿這個 hashes 進行 fuzzing
- Fuzzable 的值改成 FuzzableX，反之設定回 NA，不是 Fuzzable 的則用原本的值。

-> 目前找到了不重複的 hash12 和實作了使用 index 取出 body 內的 key,value

明天要做 -> 依照 hashSet 遍歷 hashes, 如果不在 Set 裡面的 key 直接拔掉，反之如果在裡面就依照 refleactMap 裡面的參數修改 hashes[hash][body] 裡面的值

    -> 這有個問題是會不會 UrlA 和 UrlB 都有可用的 api HashA，其中 UrlA 的 iString 為 F0, F1。而 UrlB 的 iString 為 F1 F2。
        -> 有沒有可能 Fuzzing F0 後會讓 UrlB Failed?
            -> 應該有可能
                -> 如果直接針對去重複的 hash 接 Fuzzingbook -> 可能有些值太長反而讓 req failed
                    -> 這層過濾有機會能提高 req 成功機率，也可能完全無效

>>>>>>>>  應該要先直接去重複後用 hashes 接 Fuzzingbook 才對 


## 25/11/13

目前在主頁的 title 處發現注入一個可以點擊的東西 疑似 tag 會觸發，但是點擊下去後沒有反應，不過這是一個好的訊號!!

```
'<object data="data:text/html;base64,PHNjcmlwdD5hbGVydCgxKTwvc2NyaXB0Pg" type="text/html"data:text/html,<script>Alert(2)</script>" type="text/html"data:application/xml,<sVG ondblclick=open(2)> %3E<image href=javascript:alert(2) onwheel=open(2)> </sVG><" type="application/xml"data:text/html,<script>Open(2)</script>" type="text/html"data:text/html;base64,PHNjcmlwdD5hbGVydCgxKTwvc2NyaXB0Pg" type="text/html 2 <object>`
```

成功注入了 shit，但我找不到是哪裡注入的 Payload 太長了

找到了，資訊都在 ReasearchPic 裡面，可以停下來稍微整理資料和老師報告了

目前執行順序是 Cralwer/main.py -> injectharmless/ArgParse.py -> injectharmless/main.py -> traverse/main.py -> Fuzzing/Fuzzer.py

能順利注入 Payload，並且真的有找到 API 是能夠觸發 Stored XSS 的，不過現在的問題是 JS 注入後網頁一定會出現，會不會執行又是一回事所以不好做事後的檢測，
即使輸入可識別字串也不能確定 Payload 到底會不會被執行，還有的問題是我的 Fuzzer 產出來的 Payload 不一定是可用的 XSS Payload

Todo
-> 修改 Fuzzer 讓他產出來的東西一定可用
-> 找到比較適當的紀錄手段