# 25/08/28
- 在使用爬蟲找尋網頁的過程中，點擊按鈕後會記錄下這次發送的 request body。但是如果 API 包含 Token，同時這次自動化取得了 200 response，或許會讓這個 Token 失效，
進而導致後面重複使用 Token 的請求失敗。

    - 可能的解法

        - 使用 mitmproxy 看到 POST 請求後不送出，直接攔截下來。mitmproxy 可以自主回應 200 ok，這樣他就是還沒被用過的 Token。
            - (問)如果每個 API 能獨立做測試，這表示不需要前面瀏覽的狀態也能成功送出 API，那還大費周章建一棵樹的理由是甚麼?只要有API編號或可辨識字串就可以了吧?應該只需要API清單?
                - 這樣第二次做回測也就是察看 Response 時就有可以查閱的路徑，而且透過一條穩定的路由能夠快速產出完整的攻擊向量和報告

        - 在每次要送出請求時，都依照路由圖 run 一遍，產生全新的 Token 後在送出請求
            - (缺)實作上可能要依靠 Selenium 和 Requests 混用，感覺非常麻煩且很慢。
            - (優)這個能契合我所想要傳達的 Stateful 的概念，也才能凸顯 Api Tree 的功用所在。

    - 目前

        - 理論上 Token 沒被用過應該是能用的，而且網頁表單的輸入應該不會有太多是直接點擊可以 200 ok 的行為，我覺得先按照現有的演算法實作看看結果在下決定。
        如果能夠順利觸發的 API 很少，就要考慮改成用上面的解法實作。

# 25/09/04
- 我發現測到後面 Session 會過期，可能需要定期更換 Session
- 看能不能修改成 Ctrl + C 後一定會寫入 JSON

# 25/09/09
- 看能不能修改成 Ctrl + C 後一定會寫入 JSON [V]
- 我發現測到後面 Session 會過期，可能需要定期更換 Session (定期刷新的邏輯反而不好處理，在 Joomla admin pannel 可以設定 Session lifetime.) [?]
- admin pannel 的 installer 頁面不要動，這邊有太多插件是無關的  (多一個 function 設定是某個網頁不要動)
- 多一個功能是設定某些 API 不要動，例如 logout 的 api 不要動 [V]
- 在 Processing 處加上進度條 (已完成/未完成) 方便我查看目前進度。 (Grok 版有)
    - 有沒有可能加上 thread 加快速度 ?
- 在測試階段時點擊按鈕少了 required 值會不會可能降低能探索到的深度 (這好像無所謂)
- string index out of range (QQ)

- 目前主要問題 -> 樹狀結構建立問題 (應該要選擇一個 stable 的 cursor 或 grok 的版本，先確定可以正常掃描網站，然後自己改樹狀結構的寫入邏輯)
    1. 建立樹狀結構根結點選擇問題 (Wire.py 版本)
    2. 樹狀結構無法呈現全部的 urls，例如 admin pannel 裡面有一堆 url，但即使檢測到也掃描過 url，但就是沒辦法被記載入樹狀結構 (Grok.py 版本)


# 25/09/10

建立四個物件
- [V] VisitedUrl (set), 用來儲存走訪過什麼 url，不要出現環(原本的就可以)
- [V] ParentMap (dict), {url:parent} 可以在當前 url 在找尋網頁中的 <a> tag 時匯入，例如 index.php 網頁中有 article 就會是當 driver 在 /index.php 時匯入資訊 {'/article':'/index.php'}
- UrlToApis (dict), {url:apis:{}} 因為每個 url 底下的 api 不會具有深度的特性，所以直接另外取出來儲存。
- [V] UrlToNode (dict), {url:children} 這個在最後建樹階段時建立，是在未來方便查詢這個樹所建立的 index 表
                    {str:list}
- 建樹: 走訪 ParentMap，因為在掃描過程使用的是 BFS，所以 ParentMap 匯入的 Key 值順序會契合匯入的節點 Lavel，不會有提前需要匯入較深節點的問題，所以 Tree 會製作成
{
    rooturl:
        children: {
            suburl1: {
                children :{
                    subsuburl1 : {

                    }
                }
            }, 
            suburl2: {
                children :{
                    subsuburl1 :{

                    }
                }
            }
            ...
        }
}

在建樹時因為 ParentMap 的插入順序，會讓 suburl 層先被匯入(再搭配指向的 parent 可以確定應該將節點放在樹的何處)，
這時候就能在 UrlToNode 裡面建立第一個值 {root:[suburl1, suburl2]}，當 suburl1 都走過一次後會開始走到 suburl1 會建立為 {root:[suburl1, suburl2], suburl1:[]}
如此往復就能把 node 資訊完好無缺的存入 UrlToNode 裡面。

這邊只存 url 即可，因為 api 儲存僅收錄當前頁面中的，api，不需要考慮深度問題，所以僅需要一個扁平的 {url:{apis} }(裡面儲存的資料必須要包含整個 request body, header 等)

未來要查詢時就能透過 O(1) 的時間查詢到任何 url 之下的 children，同時能夠透過 url 查詢 api，甚至也能透過 ParentMap 查詢 previous 的 node。

(以上全部 url 皆已絕對路徑儲存，修改 Grok 那份 code 比較快，因為裡面還有 exclude url 的功能在)

目前 Grok 版本已經能正確輸出 tree.json，但還沒讓 Api 也 sync 起來

## 25/09/11 
- 把 API 的部分用之前的 AnalyseInput 和 Requests_handler 整理好再匯入 Json
    - 目前是 BFS 的方式，如果中途突然 get 出去抓 forms 的資訊可能導致邏輯出錯，到時候 bug 會修不完
    - 應該要嘗試如何把抓 API 的動作混合進目前的業務流程中。
    - 為了方便直接透過 url 檢索，結構應該要是這樣
    {
        url1 :{
            apiurl1: {
                'body':body.text,
                'method':method,
                'headers':headers
            },
            apiurl2: {
                ...
            }
        }
    }
    1. 先 init requests_handler, 給他 cookies/session 
    2. 我需要 AllTags, body_text, method, headers, url, 
    - 要插入 Payload 的產生邏輯要寫在 BuildData，那邊非常快速可以寫完

    -> 目前可以產收 API json，但是會有一些奇怪的情況，例如 req.body 是有東西的，但是進行 BuildData 時偵測 forms 卻偵測不到
        - 有可能那個頁面就是沒有 forms?
            -  有可能，例如 administrator 頁面一堆按鈕可以 POST 但自己本身沒有 forms
    - 目前 API 樹應該堪用了，但有疑問要解決
        - administrator 這邊真的沒有半個 API 需要紀錄?
        - 可能要跑看看 50 - 100 個結果再來確定是否正確

## 25/09/16
- 有沒有可能用 multithreads 加速?
    -> 有，但非常困難，因為原先使用 BFS，然後在 BFS 的特性中內涵樹狀結構的階層順序。如果改以 multithreads 
    可能會影響這個順序進而導致建樹階段出現錯誤。
    -> Python 內建的 queue.Queue 好像是 thread-safe，內建 lock 可以解決 multithreads 的問題

## 25/10/05
-> 目前考慮把 Requests 和 Selenium 模組分開來進行，爬蟲執行完後要把狀態交給 Requests 所以要把 Cookies Export 出來
    -> Load Successfully
-> 現在要測試注入無害的 requests，應該要忽略 login/logout
-? 如果有 404 的頁面是不是不要紀錄比較好 ?

## 25/10/07
-> 目前 Apis 清單內會抓到 login，這個應該要處理掉
-> Apis 清單內有一堆是空的，我猜是 not found 頁面很多是 POST，但是實際去看頁面後卻甚麼都抓不到，導致 apis.json 抓到很多空值
    -> 要把這些空值過濾掉 不然能使用的 Apis 太少
-> 目前有很多頁面是 not found 的，要看情況弄掉
    -> driver 可以吃到 status code 了，也不會記錄在 process 裡面，但是 Json 還是會記錄到，可能要思考一下這樣是好還是不好
-> 發現新問題！！！！，如果執行過某個API後才新增網頁呢? 那這個網頁要怎麼確認?
    -> 目前 Submit an article 後就會出現新的頁面在外面
-> Fuzzable 有的時候能正常記錄在 Apis.json, 但有時候又不行，不確定為何

-> 有時候沒辦法登入有時候可以，本來以為是 headers 的問題，後來發現 headers 隨便抓到 google 的也能登入，表示跟 headers 無關。我猜是 Session 有沒有過期導致的。
    -> 修改 System 那邊的 Session lifetime

************
當務之急是先讓 Requests 能夠正常發出，先嘗試自動化使用 submit-an-article
    -> submit-an-article 那個的 api 裡面要是 task.save 才能順利發出文章
        -> 但是 task.save 的 apis 有包含在我的 apis 清單裡面，理論上不用修改其他內容
        -> 原本發現 Fuzzable 不能正確標上去，現在標上去了。
        -> 目前的版本可以從 Crawler 接到 Apis.json 後透過 requests 成功送出請求

## 25/10/08

目前發現執行爬蟲後為了檢測 status code，導致部分 requests 無法正常地被 driver.get 捕捉到(因為僅用弱的 time.sleep())
這樣會讓一些網頁檢測不到 driver.requests 進而出現錯誤。
-> 不對啊完蛋，我發現 404 的頁面出來的原因應該是因為 urljoin 出錯，/administrator/index.php 的頁面的 href 直接合併到了 /index.php 後面了
```
path:  http://192.168.11.129:8080/administrator/index.php
t:  index.php?option=com_joomlaupdate
url: http://192.168.11.129:8080/index.php?option=com_joomlaupdate
path:  http://192.168.11.129:8080/administrator/index.php
t:  index.php?option=com_joomlaupdate
url: http://192.168.11.129:8080/index.php?option=com_joomlaupdate
```
joomla 在搞= =
-> 目前是當作 Joomla 的 Special case 處理，如果未來延伸到要用到其他系統上要注意，邏輯寫在 GetMergeUrl 處

Processing (done/pending) 121/182:   http://192.168.11.129:8080/administrator/index.php?option=com_users&view=debuggroup&group_id=6
[9716:15964:1008/141347.490:ERROR:ui\latency\latency_info.cc:112] CompositorFrameSinkSupport::MaybeSubmitCompositorFrame, LatencyInfo vector size 667 is too big.

目前在 Processing 到 121 時會沒辦法 saving，就嘗試用 118 吧
在 118 的時候會有一個很詭異的錯誤
Reached limit 118, saving and exiting...
Driver closed.
Save failed: 'http://192.168.11.129:8080/administrator/index.php?option=com_admin&view=profile&layout=edit&id=255'

-> 在 50 筆的時候有部分 body 的解碼錯了QQ
    -> 好像是因為 GetNext 太快了，我改回 time.sleep(3) 就正常回來了